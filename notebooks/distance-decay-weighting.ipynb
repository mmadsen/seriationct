{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Distance-Decay-Weighting-Options\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Distance Decay Weighting Options</a></div><div class=\"lev2\"><a href=\"#Define-Utility-Functions\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Define Utility Functions</a></div><div class=\"lev2\"><a href=\"#Trial-Analysis\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Trial Analysis</a></div><div class=\"lev3\"><a href=\"#Linear-Distance-Decay\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Linear Distance Decay</a></div><div class=\"lev3\"><a href=\"#Exponential-Decay-with-Scale-Factor\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Exponential Decay with Scale Factor</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:23:40.129694",
     "start_time": "2016-02-27T14:23:39.595455"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T13:46:09.189795",
     "start_time": "2016-02-27T13:46:09.187732"
    }
   },
   "source": [
    "# Distance Decay Weighting Options #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea behind a approximate-nearest-neighbor interaction network is that most of the inter-community links are fairly local, i.e., span short geographic distances.  This should create a regional interaction network where innovations do not spread instantly across the region but instead pass through intermediate communites as they diffuse across a region, the presence of occasional long-distance connection notwithstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is what kind of distance decay kernel and weighting scheme leads to randomly constructed networks.  Linear decay (i.e., simply scaling the distances into probabilities) leads to min/max/mean distances that are smaller than potential distances (i.e., distances between vertices in the graph whether there are any edges linking them), but not much smaller.  This notebook is an exploration of distance kernels to determine how to scale the decay within an abstract RTN model such that we end up with average realized edge distances that are only a fraction of the maximum distances possible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Utility Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T13:44:25.656419",
     "start_time": "2016-02-27T13:44:25.644379"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_distance_map_to_self(g):\n",
    "    \"\"\"\n",
    "    Constructs twolevel map of distances from each node in the current slice, to other nodes in the same slice\n",
    "\n",
    "    :param g:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dist_map = dict()\n",
    "    for n,d in g.nodes_iter(data=True):\n",
    "        n_label = g.node[n]['label']\n",
    "        dist_map[n_label] = dict()\n",
    "        n_x = int(g.node[n]['xcoord'])\n",
    "        n_y = int(g.node[n]['ycoord'])\n",
    "\n",
    "        for l,d in g.nodes_iter(data=True):\n",
    "            # we don't need self distances\n",
    "            if l == n:\n",
    "                continue\n",
    "            l_label = g.node[l]['label']\n",
    "            l_x = float(g.node[l]['xcoord'])\n",
    "            l_y = float(g.node[l]['ycoord'])\n",
    "            dist = math.sqrt(pow(l_y - n_y, 2) + pow(l_x - n_x, 2))\n",
    "            dist_map[n_label][l_label] = dist\n",
    "\n",
    "    return dist_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T13:44:42.154027",
     "start_time": "2016-02-27T13:44:42.131863"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_community_distance_statistics(g, ignore_actual=True):\n",
    "    \"\"\"\n",
    "    Calculates both the min/max/average edge distance, taking each edge into account only once,\n",
    "    and the possible values for these given all pairs of vertices whether linked or not.  There\n",
    "    are enough returned statistics that the function returns a dict with keys:  min_actual,\n",
    "    mean_potential, etc.\n",
    "\n",
    "    For actual values, pass \"ignore_actual=False\".  This allows the function to be used on\n",
    "    sets of vertices with geographic coordinates but before we actually wire the edges.\n",
    "\n",
    "    :param g:\n",
    "    :param: ignore_actual - boolean to not calculate statistics for actual edges, since we might not have any yet.\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    actual_distances = []\n",
    "    potential_distances = []\n",
    "\n",
    "    nodes = g.nodes()\n",
    "    for i,j in itertools.product(nodes, nodes):\n",
    "        if i == j:\n",
    "            continue\n",
    "        i_x = float(g.node[i]['xcoord'])\n",
    "        i_y = float(g.node[i]['ycoord'])\n",
    "        j_x = float(g.node[j]['xcoord'])\n",
    "        j_y = float(g.node[j]['ycoord'])\n",
    "        dist = math.sqrt(pow(i_y - j_y, 2) + pow(i_x - j_x, 2))\n",
    "        if g.has_edge(i,j):\n",
    "            actual_distances.append(dist)\n",
    "        potential_distances.append(dist)\n",
    "\n",
    "    # log.debug(\"actual dist: %s\", actual_distances)\n",
    "    # log.debug(\"potential dist: %s\", potential_distances)\n",
    "\n",
    "    res = dict()\n",
    "    if ignore_actual == False:\n",
    "        res['actual_min'] = np.amin(actual_distances)\n",
    "        res['actual_max'] = np.amax(actual_distances)\n",
    "        res['actual_mean'] = np.mean(actual_distances)\n",
    "        res['edge_density'] = float(len(actual_distances)) / float(len(potential_distances))\n",
    "    res['potential_min'] = np.amin(potential_distances)\n",
    "    res['potential_max'] = np.amax(potential_distances)\n",
    "    res['potential_mean'] = np.mean(potential_distances)\n",
    "\n",
    "    # log.debug(\"res: %s\", res)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:08:03.291528",
     "start_time": "2016-02-27T14:08:03.279789"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_weighted_node_lists_exponential_decay(node, dist_map, average_potential_dist, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Build a list of neighbors for the focal node, and a list of probability weights where\n",
    "    the probability is linear in the inverse of distance (smaller distances equal larger weights)\n",
    "\n",
    "    :param node:\n",
    "    :param dist_map:\n",
    "    :param average_potential_dist:\n",
    "    :param alpha: scaling factor, governs how steep the decay curve is.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    node_list = []\n",
    "    weight_list = []\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for label,d in dist_map[node].items():\n",
    "        distances.append(d)\n",
    "        node_list.append(label)\n",
    "\n",
    "    n_distances = np.asarray(distances)\n",
    "    # log.debug(\"distances: %s\", n_distances)\n",
    "\n",
    "    # raw exponential weights are exp(-distance[i]/average_potential_dist)\n",
    "    raw_weights = np.exp(alpha * (n_distances * -1)/average_potential_dist)\n",
    "    #log.debug(\"raw exponential weights: %s\", raw_weights)\n",
    "\n",
    "    total_weights = np.sum(raw_weights)\n",
    "\n",
    "    scaled_weights = raw_weights / total_weights\n",
    "    #log.debug(\"scaled exponential weights: %s\", scaled_weights)\n",
    "\n",
    "    return (node_list, scaled_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:41:55.017783",
     "start_time": "2016-02-27T14:41:54.998326"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_weighted_node_lists_linear_decay(node, dist_map, max_x_coord, max_y_coord):\n",
    "    \"\"\"\n",
    "    Build a list of neighbors for the focal node, and a list of probability weights where\n",
    "    the probability is linear in the inverse of distance (smaller distances equal larger weights)\n",
    "\n",
    "    :param node:\n",
    "    :param dist_map:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    node_list = []\n",
    "    weight_list = []\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for label,d in dist_map[node].items():\n",
    "        distances.append(d)\n",
    "        node_list.append(label)\n",
    "\n",
    "    n_distances = np.asarray(distances)\n",
    "    # log.debug(\"distances: %s\", n_distances)\n",
    "\n",
    "    # divisor is the maximum possible distance in the region, which is the distance from origin\n",
    "    # diagonally to the max x and max y coordinate.\n",
    "    n_total = math.sqrt(max_x_coord**2 + max_y_coord**2)\n",
    "    frac_distances = n_distances / n_total\n",
    "    # log.debug(\"frac distances: %s\",frac_distances)\n",
    "\n",
    "    frac_distances = 1.0 - frac_distances\n",
    "    # log.debug(\"inverse frac distances: %s\", frac_distances)\n",
    "\n",
    "    total_frac_distance = np.sum(frac_distances)\n",
    "    frac_distances = frac_distances / total_frac_distance\n",
    "\n",
    "    # log.debug(\"scaled frac distances: %s\", frac_distances)\n",
    "\n",
    "    total_weights = sum(frac_distances)\n",
    "\n",
    "    return (node_list, frac_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:48:21.595067",
     "start_time": "2016-02-27T14:48:21.573440"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_linear_weighted_edges_to_slice(g, max_x_coord, max_y_coord):\n",
    "    \"\"\"\n",
    "    Takes an empty graph with N nodes, and given parameters for mean/sd node degree, and generates\n",
    "    random inverse-distance-weighted edges, so that neighbors are preferentially geographically close.\n",
    "\n",
    "    :param g:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # generate a random number of edges for each vertex, drawn from a lognormal distribution, clipped to 1 on the\n",
    "    # lower limit (no isolated vertices) and the number of populations minus one on the upper.\n",
    "    edges_per_vertex = np.clip(np.random.lognormal(1.5,\n",
    "                                                   0.2, size = 32), \n",
    "                               1, 31).astype(np.int64)\n",
    "    # log.debug(\"edges per vertex: %s\", edges_per_vertex)\n",
    "    # to allow us to index the number of edges, poor man's generator\n",
    "    edge_ix = 0\n",
    "    dist_map = build_distance_map_to_self(g)\n",
    "    # we need the latter because we usually prefer to deal in node labels, but edge wiring requires IDs.\n",
    "    label_map = build_map_from_vertex_label_to_id(g)\n",
    "    dist_stat = calculate_community_distance_statistics(g, ignore_actual=True)\n",
    "\n",
    "    for v,d in g.nodes_iter(data=True):\n",
    "        num_neighbors = edges_per_vertex[edge_ix]\n",
    "        # reduce by the number of existing edges\n",
    "        num_neighbors -= len(g.neighbors(v))\n",
    "        if num_neighbors < 1:\n",
    "            continue\n",
    "\n",
    "        # log.debug(\"selecting %s neighbors\", num_neighbors)\n",
    "\n",
    "        n_label = g.node[v]['label']\n",
    "        other_node_list, weights = build_weighted_node_lists_linear_decay(n_label, dist_map, max_x_coord, max_y_coord)\n",
    "        # log.debug(\"other node list: %s\", other_node_list)\n",
    "        # log.debug(\"weights: %s\", weights)\n",
    "\n",
    "        random_neighbor_list = np.random.choice(other_node_list, size=num_neighbors, p=weights)\n",
    "        # log.debug(\"selected neighbors: %s\", random_neighbor_list)\n",
    "\n",
    "        # go over random neighbors and wire them with edges\n",
    "        for neighbor_label in random_neighbor_list:\n",
    "            g.add_edge(v, label_map[neighbor_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:48:05.503482",
     "start_time": "2016-02-27T14:48:05.485655"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def assign_exponential_weighted_edges_to_slice(g, alpha):\n",
    "    \"\"\"\n",
    "    Takes an empty graph with N nodes, and given parameters for mean/sd node degree, and generates\n",
    "    random inverse-distance-weighted edges, so that neighbors are preferentially geographically close.\n",
    "\n",
    "    :param g:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # generate a random number of edges for each vertex, drawn from a lognormal distribution, clipped to 1 on the\n",
    "    # lower limit (no isolated vertices) and the number of populations minus one on the upper.\n",
    "    edges_per_vertex = np.clip(np.random.lognormal(1.5,0.2, size = 32), 1, 31).astype(np.int64)\n",
    "    # log.debug(\"edges per vertex: %s\", edges_per_vertex)\n",
    "    # to allow us to index the number of edges, poor man's generator\n",
    "    edge_ix = 0\n",
    "    dist_map = build_distance_map_to_self(g)\n",
    "    # we need the latter because we usually prefer to deal in node labels, but edge wiring requires IDs.\n",
    "    label_map = build_map_from_vertex_label_to_id(g)\n",
    "    dist_stat = calculate_community_distance_statistics(g, ignore_actual=True)\n",
    "\n",
    "    for v,d in g.nodes_iter(data=True):\n",
    "        num_neighbors = edges_per_vertex[edge_ix]\n",
    "        # reduce by the number of existing edges\n",
    "        num_neighbors -= len(g.neighbors(v))\n",
    "        if num_neighbors < 1:\n",
    "            continue\n",
    "\n",
    "        # log.debug(\"selecting %s neighbors\", num_neighbors)\n",
    "\n",
    "        n_label = g.node[v]['label']\n",
    "        other_node_list, weights = build_weighted_node_lists_exponential_decay(n_label, dist_map, dist_stat['potential_mean'], alpha)\n",
    "        # log.debug(\"other node list: %s\", other_node_list)\n",
    "        # log.debug(\"weights: %s\", weights)\n",
    "\n",
    "        random_neighbor_list = np.random.choice(other_node_list, size=num_neighbors, p=weights)\n",
    "        # log.debug(\"selected neighbors: %s\", random_neighbor_list)\n",
    "\n",
    "        # go over random neighbors and wire them with edges\n",
    "        for neighbor_label in random_neighbor_list:\n",
    "            g.add_edge(v, label_map[neighbor_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:48:48.243258",
     "start_time": "2016-02-27T14:48:48.237649"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_maximum_regional_coordinates(aspectratio, numpopulations):\n",
    "    \"\"\"\n",
    "    Given the aspect ratio desired for the regional model (square, long thin, as defined by the ratio of sides),\n",
    "    and the number of populations, deliver an XY coordinate system (in terms of maxima) that yields a 1% ratio\n",
    "    of occupied space if each population takes up a 10x10 unit in the coordinate system.\n",
    "\n",
    "    :param aspectratio:\n",
    "    :param numpopulations:\n",
    "    :return: tuple of maximum_x, maximum_y\n",
    "    \"\"\"\n",
    "    occupied_space = 100. * numpopulations\n",
    "    total_area = occupied_space / 0.01\n",
    "\n",
    "    side = math.sqrt(total_area / aspectratio)\n",
    "    x = side\n",
    "    y = aspectratio * side\n",
    "\n",
    "    return (x,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:48:55.233080",
     "start_time": "2016-02-27T14:48:55.229052"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_map_from_vertex_label_to_id(g):\n",
    "    \"\"\"\n",
    "    Keep a map from vertex label to node ID, since we don't want to relabel the nodes in this context\n",
    "\n",
    "    :param g:\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    label_map = dict()\n",
    "    for n,d in g.nodes_iter(data=True):\n",
    "        label_map[g.node[n]['label']] = n\n",
    "\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial Analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:39:13.906361",
     "start_time": "2016-02-27T14:39:13.903479"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n",
      "1600.0\n"
     ]
    }
   ],
   "source": [
    "(max_x_coord, max_y_coord) = find_maximum_regional_coordinates(8.0, 32)\n",
    "print max_x_coord\n",
    "print max_y_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:40:51.930464",
     "start_time": "2016-02-27T14:40:51.630495"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = nx.read_gml(\"foo-001.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm grabbing a regional graph with a 8.0 aspect ratio, meaning that the region is 8x longer than it is wide, to make a long narrow space.  The overall size is governed by ensuring that the 32 populations, if they each take up 10x10 area, make up no more than 1% of the total area of the region.  \n",
    "\n",
    "First, we make a copy of the graph and remove all the edges, because we're going to rewire it with different distance decay functions.  I do this in a way that leaves node information intact, since that's where the geographic coordinates are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:40:55.381810",
     "start_time": "2016-02-27T14:40:55.374422"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'potential_mean': 459.66243998708808, 'potential_min': 11.180339887498949, 'potential_max': 1523.4992615685771}\n"
     ]
    }
   ],
   "source": [
    "edge_list = g.edges()\n",
    "g.remove_edges_from(edge_list)\n",
    "print calculate_community_distance_statistics(g, ignore_actual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:32:18.751188",
     "start_time": "2016-02-27T14:32:18.744526"
    }
   },
   "source": [
    "These are the reference values for how far away graph nodes are:  at a minimum, about 11 units, max about 1523, with an average of about 460.  Let's see how we can get **wired** edges that yield an actual network with smaller distances between the wired nodes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Distance Decay ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:44:21.962911",
     "start_time": "2016-02-27T14:44:21.958431"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_g = g.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:48:59.221850",
     "start_time": "2016-02-27T14:48:59.206235"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assign_linear_weighted_edges_to_slice(linear_g, max_x_coord, max_y_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:49:38.646926",
     "start_time": "2016-02-27T14:49:38.639571"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edge_density': 0.11693548387096774, 'potential_max': 1523.4992615685771, 'actual_mean': 452.63333549773301, 'actual_max': 1315.8632147757608, 'potential_mean': 459.66243998708808, 'actual_min': 30.083217912982647, 'potential_min': 11.180339887498949}\n"
     ]
    }
   ],
   "source": [
    "print calculate_community_distance_statistics(linear_g, ignore_actual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the mean distance isn't much smaller than the potential mean distance, so we haven't really shrunk the locality of the graph yet.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:53:57.491703",
     "start_time": "2016-02-27T14:53:57.489895"
    }
   },
   "source": [
    "### Exponential Decay with Scale Factor ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T15:00:01.010494",
     "start_time": "2016-02-27T15:00:00.988249"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edge_density': 0.18548387096774194, 'potential_max': 1523.4992615685771, 'actual_mean': 321.16227979969773, 'actual_max': 1184.7193760549374, 'potential_mean': 459.66243998708808, 'actual_min': 11.180339887498949, 'potential_min': 11.180339887498949}\n"
     ]
    }
   ],
   "source": [
    "exponential_1 = g.copy()\n",
    "assign_exponential_weighted_edges_to_slice(exponential_1, 1.0)\n",
    "print calculate_community_distance_statistics(exponential_1, ignore_actual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:58:28.295460",
     "start_time": "2016-02-27T14:58:28.276108"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edge_density': 0.1592741935483871, 'potential_max': 1523.4992615685771, 'actual_mean': 254.53850706974896, 'actual_max': 1140.8492450801728, 'potential_mean': 459.66243998708808, 'actual_min': 22.0, 'potential_min': 11.180339887498949}\n"
     ]
    }
   ],
   "source": [
    "exponential_2 = g.copy()\n",
    "assign_exponential_weighted_edges_to_slice(exponential_2, 2.0)\n",
    "print calculate_community_distance_statistics(exponential_2, ignore_actual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:59:29.683041",
     "start_time": "2016-02-27T14:59:29.662365"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edge_density': 0.16532258064516128, 'potential_max': 1523.4992615685771, 'actual_mean': 183.80658176230142, 'actual_max': 636.94426757762722, 'potential_mean': 459.66243998708808, 'actual_min': 11.180339887498949, 'potential_min': 11.180339887498949}\n"
     ]
    }
   ],
   "source": [
    "exponential_3 = g.copy()\n",
    "assign_exponential_weighted_edges_to_slice(exponential_3, 3.0)\n",
    "print calculate_community_distance_statistics(exponential_3, ignore_actual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-27T14:59:33.038606",
     "start_time": "2016-02-27T14:59:33.020077"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edge_density': 0.1975806451612903, 'potential_max': 1523.4992615685771, 'actual_mean': 168.56577527337745, 'actual_max': 697.20728624993581, 'potential_mean': 459.66243998708808, 'actual_min': 11.180339887498949, 'potential_min': 11.180339887498949}\n"
     ]
    }
   ],
   "source": [
    "exponential_4 = g.copy()\n",
    "assign_exponential_weighted_edges_to_slice(exponential_4, 4.0)\n",
    "print calculate_community_distance_statistics(exponential_4, ignore_actual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "toc_cell": true,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
